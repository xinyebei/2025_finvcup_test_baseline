'''
# author: Zhiyuan Yan
# email: zhiyuanyan@link.cuhk.edu.cn
# date: 2023-0706
# description: Class for the XceptionDetector

Functions in the Class are summarized as:
1. __init__: Initialization
2. build_backbone: Backbone-building
3. build_loss: Loss-function-building
4. features: Feature-extraction
5. classifier: Classification
6. get_losses: Loss-computation
7. get_train_metrics: Training-metrics-computation
8. get_test_metrics: Testing-metrics-computation
9. forward: Forward-propagation

Reference:
@inproceedings{rossler2019faceforensics++,
  title={Faceforensics++: Learning to detect manipulated facial images},
  author={Rossler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1--11},
  year={2019}
}
'''

import datetime
import logging
import os
from collections import defaultdict
from typing import Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from metrics.base_metrics_class import calculate_metrics_for_train
from sklearn import metrics
from torch.nn import DataParallel
from torch.utils.tensorboard import SummaryWriter
from transformers import AutoProcessor, CLIPModel, ViTConfig, ViTModel

from loss.cross_entropy import CrossEntropyLoss
logger = logging.getLogger(__name__)

class CLIPDetector(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.backbone = self.build_backbone(config)
        # self.head = nn.Linear(1024, 2)  # for CLIP-large-14
        self.head = nn.Linear(768, 2) # for CLIP-base-16
        self.loss_func = self.build_loss(config)

        if config.get("finetune", False):
            self.load_ckpt(config['pretrained'])
    
        
        
    def build_backbone(self, config):
        # please download the ckpts from the below link
        
        # use CLIP-base-16
        _, backbone = get_clip_visual(model_name="openai/clip-vit-base-patch16")        
        
        # use CLIP-large-14
        # _, backbone = get_clip_visual(model_name="openai/clip-vit-large-patch14")      
        return backbone
    
    def build_loss(self, config):
        # prepare the loss function
        loss_class = CrossEntropyLoss
        loss_func = loss_class()
        return loss_func
    
    def features(self, data_dict: dict) -> torch.tensor:
        feat = self.backbone(data_dict['image'])['pooler_output']
        return feat

    def classifier(self, features: torch.tensor) -> torch.tensor:
        return self.head(features)
    
    def get_losses(self, data_dict: dict, pred_dict: dict) -> dict:
        label = data_dict['label']
        pred = pred_dict['cls']
        loss = self.loss_func(pred, label)
        loss_dict = {'overall': loss}
        return loss_dict
    
    def get_train_metrics(self, data_dict: dict, pred_dict: dict) -> dict:
        label = data_dict['label']
        pred = pred_dict['cls']
        # compute metrics for batch data
        auc, eer, acc, ap = calculate_metrics_for_train(label.detach(), pred.detach())
        metric_batch_dict = {'acc': acc, 'auc': auc, 'eer': eer, 'ap': ap}
        return metric_batch_dict

    def forward(self, data_dict: dict, inference=False) -> dict:
        # get the features by backbone
        features = self.features(data_dict)
        # get the prediction by classifier
        pred = self.classifier(features)
        # get the probability of the pred
        prob = torch.softmax(pred, dim=1)[:, 1]
        # build the prediction dict for each output
        pred_dict = {'cls': pred, 'prob': prob, 'feat': features}

        return pred_dict

def get_clip_visual(model_name = "openai/clip-vit-base-patch16"):
    processor = AutoProcessor.from_pretrained(model_name)
    model = CLIPModel.from_pretrained(model_name)
    return processor, model.vision_model

def get_vit_model(model_name = "google/vit-base-patch16-224-in21k"):
    #processor = AutoProcessor.from_pretrained(model_name)
    configuration = ViTConfig(
        image_size=224,
    )
    model = ViTModel.from_pretrained(model_name, config=configuration)
    return None, model
